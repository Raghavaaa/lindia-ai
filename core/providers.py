"""
AI Provider routing system with fallback support.
Supports INLEGALBERT, DeepSeek, and Grok providers.
"""
import time
import httpx
from typing import Dict, List, Optional, Tuple
from enum import Enum

from core.config import settings
from core.logger import logger


class Provider(str, Enum):
    """Supported AI providers."""
    INLEGALBERT = "INLEGALBERT"
    DEEPSEEK = "DeepSeek"
    GROK = "Grok"


class ProviderRouter:
    """
    Routes requests to AI providers with fallback support.
    Provider order is configurable via environment variables.
    """
    
    def __init__(self):
        self.primary_provider = settings.PRIMARY_PROVIDER
        self.fallback_providers = settings.fallback_provider_list
        self.client = httpx.AsyncClient(timeout=30.0)
    
    def get_provider_order(self) -> List[str]:
        """Get ordered list of providers to try."""
        providers = [self.primary_provider] + self.fallback_providers
        return providers
    
    async def inference(
        self,
        query: str,
        context: str,
        tenant_id: str,
        request_id: str
    ) -> Dict:
        """
        Run inference with automatic fallback.
        
        Args:
            query: User query
            context: Context for the query
            tenant_id: Tenant identifier
            request_id: Request ID for tracking
            
        Returns:
            Dictionary with answer, sources, model, and latency
        """
        providers = self.get_provider_order()
        errors = []
        
        for provider in providers:
            try:
                start_time = time.time()
                
                logger.info(
                    f"Attempting inference with provider: {provider}",
                    extra={"request_id": request_id, "provider": provider, "tenant_id": tenant_id}
                )
                
                result = await self._call_provider_inference(
                    provider=provider,
                    query=query,
                    context=context,
                    tenant_id=tenant_id
                )
                
                latency_ms = round((time.time() - start_time) * 1000, 2)
                
                # Estimate cost
                estimated_tokens = len(query.split()) + len(context.split()) + len(result.get("answer", "").split())
                cost = self._estimate_cost(provider, estimated_tokens)
                
                logger.info(
                    f"Inference successful with provider: {provider}",
                    extra={
                        "request_id": request_id,
                        "provider": provider,
                        "latency_ms": latency_ms,
                        "estimated_cost": cost,
                        "tenant_id": tenant_id,
                    }
                )
                
                return {
                    "answer": result.get("answer", ""),
                    "sources": result.get("sources", []),
                    "model": provider,
                    "latency_ms": latency_ms,
                    "estimated_cost_usd": cost,
                }
                
            except Exception as e:
                error_msg = f"Provider {provider} failed: {str(e)}"
                errors.append(error_msg)
                logger.warning(
                    error_msg,
                    extra={"request_id": request_id, "provider": provider}
                )
                continue
        
        # All providers failed
        logger.error(
            "All providers failed for inference",
            extra={"request_id": request_id, "errors": errors}
        )
        raise Exception(f"All providers failed. Errors: {'; '.join(errors)}")
    
    async def _call_provider_inference(
        self,
        provider: str,
        query: str,
        context: str,
        tenant_id: str
    ) -> Dict:
        """
        Call specific provider for inference.
        This is a mock implementation - replace with actual provider API calls.
        """
        # Mock implementation - replace with real API calls
        if provider == Provider.INLEGALBERT:
            # Simulate INLEGALBERT API call
            return {
                "answer": f"Legal answer generated by INLEGALBERT for query: {query[:50]}...",
                "sources": ["IPC Section 420", "CrPC Section 145"],
            }
        elif provider == Provider.DEEPSEEK:
            # Simulate DeepSeek API call
            return {
                "answer": f"Legal analysis from DeepSeek for: {query[:50]}...",
                "sources": ["Constitution Article 21", "IPC Section 302"],
            }
        elif provider == Provider.GROK:
            # Simulate Grok API call
            return {
                "answer": f"Grok legal interpretation: {query[:50]}...",
                "sources": ["Legal precedent: Smith v. Jones"],
            }
        else:
            raise ValueError(f"Unknown provider: {provider}")
    
    async def embed(
        self,
        doc_id: str,
        text: str,
        request_id: str
    ) -> Dict:
        """
        Generate embeddings for text.
        
        Args:
            doc_id: Document identifier
            text: Text to embed
            request_id: Request ID for tracking
            
        Returns:
            Dictionary with vector_id and metadata
        """
        start_time = time.time()
        
        try:
            # Mock implementation - replace with actual embedding service
            logger.info(
                "Generating embeddings",
                extra={"request_id": request_id, "doc_id": doc_id, "text_length": len(text)}
            )
            
            # Simulate embedding generation
            vector_id = f"vec_{doc_id}_{int(time.time())}"
            
            latency_ms = round((time.time() - start_time) * 1000, 2)
            
            logger.info(
                "Embeddings generated successfully",
                extra={"request_id": request_id, "vector_id": vector_id, "latency_ms": latency_ms}
            )
            
            return {
                "vector_id": vector_id,
                "vector_meta": {
                    "doc_id": doc_id,
                    "dimensions": 768,
                    "model": "sentence-transformers/legal-bert",
                    "latency_ms": latency_ms,
                }
            }
            
        except Exception as e:
            logger.error(
                f"Embedding generation failed: {str(e)}",
                extra={"request_id": request_id, "doc_id": doc_id}
            )
            raise
    
    async def search(
        self,
        query: str,
        top_k: int,
        tenant_id: str,
        request_id: str
    ) -> Dict:
        """
        Search for similar documents using vector search.
        
        Args:
            query: Search query
            top_k: Number of results to return
            tenant_id: Tenant identifier
            request_id: Request ID for tracking
            
        Returns:
            Dictionary with search results
        """
        start_time = time.time()
        
        try:
            logger.info(
                "Performing vector search",
                extra={"request_id": request_id, "tenant_id": tenant_id, "top_k": top_k}
            )
            
            # Mock implementation - replace with actual vector store search
            results = [
                {
                    "doc_id": f"doc_{i}",
                    "score": 0.95 - (i * 0.05),
                    "metadata": {
                        "title": f"Legal Document {i}",
                        "type": "case_law",
                        "date": "2024-01-15",
                    },
                    "snippet": f"Relevant text snippet from document {i}..."
                }
                for i in range(1, min(top_k + 1, 6))
            ]
            
            latency_ms = round((time.time() - start_time) * 1000, 2)
            
            logger.info(
                "Vector search completed",
                extra={
                    "request_id": request_id,
                    "results_count": len(results),
                    "latency_ms": latency_ms
                }
            )
            
            return {
                "results": results,
                "total_count": len(results),
                "latency_ms": latency_ms,
            }
            
        except Exception as e:
            logger.error(
                f"Vector search failed: {str(e)}",
                extra={"request_id": request_id}
            )
            raise
    
    def _estimate_cost(self, provider: str, tokens: int) -> float:
        """Estimate cost based on provider and token count."""
        cost_per_1k = {
            Provider.INLEGALBERT: settings.COST_PER_1K_TOKENS_INLEGALBERT,
            Provider.DEEPSEEK: settings.COST_PER_1K_TOKENS_DEEPSEEK,
            Provider.GROK: settings.COST_PER_1K_TOKENS_GROK,
        }
        
        rate = cost_per_1k.get(provider, 0.0002)
        return round((tokens / 1000) * rate, 6)


# Global provider router instance
provider_router = ProviderRouter()

